{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnHelperDataPreparetor(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Конвертирует типы (по возможности)\n",
    "    \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, df): \n",
    "        self.df_c = df.copy()\n",
    "        for col in df.columns:\n",
    "            ser = df[col]\n",
    "            try:\n",
    "                ser2 = ser.astype(np.datetime64)\n",
    "                self.df_c[col] = ser2\n",
    "            except:\n",
    "                try:\n",
    "                    ser2 =ser.astype(int)\n",
    "                    if (ser != ser2).any():\n",
    "                        try:\n",
    "                            self.df_c[col] =ser.astype(np.float32)\n",
    "                        except:\n",
    "                            self.df_c[col] = ser\n",
    "                except:\n",
    "                    self.df_c[col] = ser                    \n",
    "        return self\n",
    "    def transform(self):        \n",
    "        try:\n",
    "            return pd.concat([self.df_c.select_dtypes(np.datetime64),\\\n",
    "                              self.df_c.select_dtypes(exclude = [np.datetime64])], 1)\n",
    "        except:\n",
    "            return self.df_c  \n",
    "        \n",
    "class SklearnHelperDataSplitter(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Класс для загрузки данных\n",
    "    \n",
    "    Параметры:\n",
    "        1) tr_size        | доля тренировочной части          | float \n",
    "        2) ho_size        | доля отложенной части             | float \n",
    "        3) shuffle        | перемешивание                     | bool\n",
    "        4) random_state   | генератор случайных чисел         | int\n",
    "        5) stratify       | стратификация целевой переменной  | bool\n",
    "        6) use_test       | исользовать тестовую часть        | bool\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, tr_size, ho_size, shuffle, random_state, stratify, use_test):\n",
    "        self.tr_size = tr_size\n",
    "        self.ho_size = ho_size\n",
    "        self.shuffle=shuffle\n",
    "        self.random_state=random_state\n",
    "        self.stratify=stratify\n",
    "        self.use_test = use_test\n",
    "    def fit(self, X, y=None):        \n",
    "        if self.use_test:\n",
    "            X_trho, self.X_te, y_trho, self.y_te = train_test_split(X, y,train_size = self.tr_size,shuffle = self.shuffle,\n",
    "                                                                    random_state = self.random_state,\\\n",
    "                                                                    stratify = y if self.stratify else None)\n",
    "            \n",
    "            self.X_tr, self.X_ho, self.y_tr, self.y_ho = train_test_split(X_trho, y_trho,train_size = self.tr_size,\\\n",
    "                                                                          shuffle = self.shuffle,\n",
    "                                                                          random_state = self.random_state,\\\n",
    "                                                                          stratify = y_trho if self.stratify else None)\n",
    "        else:\n",
    "            self.X_tr, self.X_ho, self.y_tr, self.y_ho = train_test_split(X, y,train_size = self.tr_size,\\\n",
    "                                                                          shuffle = self.shuffle,\n",
    "                                                                          random_state = self.random_state,\\\n",
    "                                                                          stratify = target if self.stratify else None)\n",
    "        return self\n",
    "    def transform(self):\n",
    "        if self.use_test:\n",
    "            return (self.X_tr, self.X_ho, self.X_te, self.y_tr, self.y_ho, self.y_te)\n",
    "        else:\n",
    "            return (self.X_tr, self.X_ho, self.y_tr, self.y_ho) \n",
    "        \n",
    "def show_eda_plots(X_tr, y_tr, time_features, cat_features, num_features, min_samples_leaf): \n",
    "    if time_features is not None:\n",
    "        print(str.upper('\\t\\t\\t\\t\\t\\t\\ttime features'))\n",
    "        for feat in time_features:\n",
    "            ser = X_tr[feat]\n",
    "            if ser.nunique()>1:        \n",
    "                y_sorted = y_tr.loc[ser.index].reset_index(drop = True).cumsum()\n",
    "                fig = plt.figure(figsize = (17, 4))\n",
    "                plt.subplot(1, 2, 1)\n",
    "                sns.kdeplot(ser, color = 'red')\n",
    "                plt.xlabel(feat)\n",
    "                plt.ylabel('density')\n",
    "                plt.title(f'{feat} kde', fontsize = 20)\n",
    "                plt.subplot(1, 2, 2)\n",
    "                y_sorted.plot(color = 'red')\n",
    "                plt.xlabel(feat)\n",
    "                plt.ylabel('cummulative target')\n",
    "                plt.title(f'cummulative target by {feat}', fontsize = 20)\n",
    "                plt.show()\n",
    "        \n",
    "    if cat_features is not None:\n",
    "        print(str.upper('\\t\\t\\t\\t\\t\\t\\tcat_features'))\n",
    "        for feat in cat_features:\n",
    "            ser = X_tr[feat]\n",
    "            if ser.nunique()>1:\n",
    "                corr = ser.to_frame().corrwith(y_tr).round(2).values.flatten()[0]\n",
    "                aggs = pd.DataFrame(np.c_[ser, y_tr]).groupby(0)[1].agg({'count', 'mean'})    \n",
    "                aggs_to_plot = aggs[aggs['count']>=min_samples_leaf]['mean']\n",
    "                fig = plt.figure(figsize = (17, 4))    \n",
    "                plt.subplot(1, 2, 1)\n",
    "                sns.kdeplot(ser, color = 'green')\n",
    "                plt.xlabel(feat)\n",
    "                plt.ylabel('density')\n",
    "                plt.title(f'{feat} kde', fontsize = 20)\n",
    "                plt.subplot(1, 2, 2)   \n",
    "                aggs_to_plot.plot(color = 'green')\n",
    "                plt.xlabel(feat)\n",
    "                plt.ylabel('mean target value')\n",
    "                plt.title(f'target by {feat}\\ncorr with target = {corr}', fontsize = 20)\n",
    "                plt.show()\n",
    "        \n",
    "    if num_features is not None:\n",
    "        print(str.upper('\\t\\t\\t\\t\\t\\t\\tnum features'))\n",
    "        for feat in num_features:    \n",
    "            ser = X_tr[feat]\n",
    "            if ser.nunique()>1:\n",
    "                fig = plt.figure(figsize = (17, 4))\n",
    "                corr = ser.to_frame().corrwith(y_tr).round(2).values.flatten()[0]\n",
    "                plt.scatter(ser, y_tr, alpha = .1)\n",
    "                plt.title(f'target by {feat}\\ncorr with target = {corr}', fontsize = 20)\n",
    "                plt.xlabel(feat)\n",
    "                plt.ylabel('target')\n",
    "    plt.show()\n",
    "    print(str.upper('\\t\\t\\t\\t\\t\\t\\tcorrelations'))   \n",
    "    corr = X_tr.select_dtypes('number').corr()    \n",
    "    plt.figure(figsize = (17, 4))\n",
    "    corr_flat = np.unique(corr.values.flatten())\n",
    "    ax1 = plt.subplot(1, 2, 1)\n",
    "    sns.kdeplot(corr_flat[corr_flat!=1], ax = ax1, color='black')\n",
    "    plt.xlabel('value')\n",
    "    plt.ylabel('density')\n",
    "    plt.title('correlations bw features', fontsize= 20)\n",
    "    ax2 = plt.subplot(1, 2, 2)\n",
    "    sns.kdeplot(X_tr.select_dtypes('number').corrwith(y_tr).values.flatten(), ax = ax2, color='black')   \n",
    "    plt.xlabel('value')\n",
    "    plt.ylabel('density')\n",
    "    plt.title('correlations bw features and target', fontsize= 20)\n",
    "    \n",
    "class SklearnHelperColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    '''выбирает колонки, отпавляемые в пайплайн'''\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.columns]\n",
    "\n",
    "class SklearnHelperLabelEncoder(TransformerMixin, BaseEstimator):\n",
    "    ''' Факторизация категорий '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        X_c = X.astype(str)\n",
    "        self.d1 = {}\n",
    "        for col in X_c.columns:\n",
    "            uniques = X_c[col].dropna().unique() \n",
    "            self.d1[col] =  dict(zip(uniques, range(len(uniques))))              \n",
    "        return self\n",
    "    def transform(self, X): \n",
    "        X_c = X.astype(str)\n",
    "        for key, value in self.d1.items():\n",
    "            X_c[key] = X_c[key].map(value)\n",
    "        return X_c\n",
    "\n",
    "class SklearnHelperTargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    ''' Кодирование категорий с помощью целевой переменной '''\n",
    "    def __init__(self, n_iter, n_folds, min_samples_leaf, seed):\n",
    "        self.n_iter = n_iter\n",
    "        self.n_folds = n_folds\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.seed = seed\n",
    "    def fit(self, X, y=None):\n",
    "        self.y_mean = y.mean()\n",
    "        _df_tr = pd.concat([X, y], 1)\n",
    "        target_col = _df_tr.columns[-1]\n",
    "        to_encode = _df_tr.columns[:-1]\n",
    "        \n",
    "        L_tr = []        \n",
    "        self.L_d_encs = []\n",
    "        for i in tqdm_notebook(range(self.n_iter)): \n",
    "            enc_tr = pd.DataFrame(index = _df_tr.index, columns = to_encode).fillna(0.0)\n",
    "            for col in to_encode:\n",
    "                for tr_idx, val_idx in KFold(self.n_folds, shuffle = True,random_state = self.seed+i)\\\n",
    "                                       .split(_df_tr):                    \n",
    "                    grp = _df_tr.iloc[tr_idx].groupby(col)[target_col].agg({'mean', 'count'}) \n",
    "                    d_enc = grp[grp['count']>=self.min_samples_leaf]['mean'].to_dict()\n",
    "                    self.L_d_encs.append((col, d_enc))\n",
    "                    to_enc_tr =_df_tr.iloc[val_idx]                    \n",
    "                    enc_tr.loc[to_enc_tr.index, col] = to_enc_tr[col].map(d_enc)                  \n",
    "            L_tr.append(enc_tr)    \n",
    "            \n",
    "        self.enc_tr =  pd.concat(L_tr, 1)\n",
    "        self._df_tr = _df_tr\n",
    "        return self    \n",
    "    def transform(self, X):\n",
    "        if np.all(X.values == self._df_tr.values):\n",
    "            return self.enc_tr.fillna(self.y_mean) \n",
    "        else:\n",
    "            df_enc = pd.DataFrame(index = X.index, columns=X.columns).fillna(0.0)\n",
    "            for feat, d in tqdm_notebook(self.L_d_encs):\n",
    "                df_enc.loc[:, feat] += X[feat].map(d) / self.n_iter\n",
    "            return df_enc.fillna(self.y_mean)\n",
    "\n",
    "class SklearnHelperFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    ''' Отбор признаков '''\n",
    "    def __init__(self, model, cv, scoring, show_progress):\n",
    "        self.model = model\n",
    "        self.cv = cv\n",
    "        self.scoring = scoring\n",
    "        self.show_progress = show_progress\n",
    "    def fit(self, X, y=None):\n",
    "        try:\n",
    "            _X = X.todense()\n",
    "        except:\n",
    "            _X =X.copy()            \n",
    "        cv_scores = []\n",
    "        for i in tqdm_notebook(range(_X.shape[1])):\n",
    "            _X_curr = _X[:, i].reshape(-1,1)\n",
    "            mean_cv_score = cross_val_score(self.model, _X_curr, y, cv =self.cv, scoring = self.scoring, n_jobs=-1).mean()\n",
    "            \n",
    "            cv_scores.append(mean_cv_score)\n",
    "        order = np.argsort(cv_scores)[::-1]\n",
    "        to_drop_before, best_features, best_cv_score = [], [], -np.inf\n",
    "        for i in tqdm_notebook(order):\n",
    "            curr_features = best_features+[i]\n",
    "            _X_curr = _X[:, curr_features]\n",
    "            mean_cv_score = cross_val_score(self.model, _X_curr, y, cv =self.cv, scoring = self.scoring, n_jobs=-1).mean()\n",
    "            if mean_cv_score>best_cv_score:\n",
    "                best_cv_score = mean_cv_score\n",
    "                best_features = curr_features\n",
    "                if self.show_progress:\n",
    "                    print('new best score = {:.5f}'.format(best_cv_score))\n",
    "            else:\n",
    "                to_drop_before.append(i)\n",
    "        while True:\n",
    "            to_drop_after = []\n",
    "            for i in tqdm_notebook(to_drop_before):\n",
    "                curr_features = best_features+[i]\n",
    "                _X_curr = _X[:, curr_features]\n",
    "                mean_cv_score = cross_val_score(self.model, _X_curr, y, cv =self.cv, scoring = self.scoring, n_jobs=-1).mean()\n",
    "                if mean_cv_score>best_cv_score:\n",
    "                    best_cv_score = mean_cv_score\n",
    "                    best_features = curr_features\n",
    "                    if self.show_progress:\n",
    "                        print('new best score = {:.5f}'.format(best_cv_score))\n",
    "                else:\n",
    "                    to_drop_after.append(i)\n",
    "            if to_drop_before == to_drop_after:\n",
    "                break\n",
    "            else:\n",
    "                to_drop_before = to_drop_after  \n",
    "        self.best_features = best_features\n",
    "        self.best_cv_score = best_cv_score\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, csc_matrix):\n",
    "            _X = X.copy()\n",
    "        else:            \n",
    "            _X = csc_matrix(X) \n",
    "        return _X[:, self.best_features]\n",
    "    \n",
    "    \n",
    "\n",
    "class SklearnHelperMetaFeaturesRegressor(TransformerMixin, BaseEstimator):    \n",
    "    def __init__(self, base_model, nfolds, seed, path_to_folder):\n",
    "        self.base_model = base_model   \n",
    "        self.nfolds = nfolds\n",
    "        self.kf = KFold(nfolds, random_state = seed, shuffle = True)\n",
    "        self.path_to_folder=path_to_folder\n",
    "    def fit(self, X, y=None):        \n",
    "        if not os.path.exists(self.path_to_folder):\n",
    "            os.makedirs(self.path_to_folder)\n",
    "        else:\n",
    "            shutil.rmtree(self.path_to_folder)\n",
    "            os.makedirs(self.path_to_folder)\n",
    "            \n",
    "        self.X = X\n",
    "        self.Z_tr = np.zeros((len(y), 1))\n",
    "        for i, (tr_idx, val_idx) in tqdm_notebook(enumerate(self.kf.split(X, y)),\\\n",
    "                                                  total = self.kf.n_splits):\n",
    "            # обучаем модель на тренировочной части\n",
    "            self.base_model.fit(X[tr_idx], y[tr_idx])\n",
    "            path_to_model= os.path.join(self.path_to_folder, f'model_{i}.pkl')            \n",
    "            joblib.dump(self.base_model, path_to_model)               \n",
    "            # предсказываем валидационную\n",
    "            self.Z_tr[val_idx, 0] = self.base_model.predict(X[val_idx]) \n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        if isinstance(self.X, (np.ndarray, np.generic)):\n",
    "            if np.array_equal(self.X, X, equal_nan=True):\n",
    "                return self.Z_tr.flatten()\n",
    "        elif isinstance(self.X, (csc_matrix)):\n",
    "            if np.array_equal(self.X[:,0].toarray().flatten(), X[:,0].toarray().flatten(), equal_nan=True):\n",
    "                return self.Z_tr.flatten()\n",
    "        predictitons=np.zeros((X.shape[0], self.nfolds))\n",
    "        for i, filename in enumerate(os.listdir(self.path_to_folder)):\n",
    "            path_to_model= os.path.join(self.path_to_folder, filename)\n",
    "            fitted_model = joblib.load(path_to_model)   \n",
    "            predictitons[:, i] = fitted_model.predict(X)\n",
    "        return np.mean(predictitons, 1).flatten()\n",
    "            \n",
    "        \n",
    "class SklearnHelperRegressorValidator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model, cv, cv_scoring, ho_scoring_func, to_tune=True):\n",
    "        self.model = model\n",
    "        self.cv = cv\n",
    "        self.cv_scoring = cv_scoring\n",
    "        self.ho_scoring_func = ho_scoring_func\n",
    "        self.to_tune = to_tune\n",
    "    def fit(self, X_tr, y_tr, X_ho, y_ho):\n",
    "        #######################################################################################################\n",
    "        def _hp_tune_v1(model, grid, X, y, cv, scoring):    \n",
    "            gs = GridSearchCV(model,param_grid=grid,cv = cv, scoring = scoring, n_jobs=-1, verbose = 1)\n",
    "            gs.fit(X, y)\n",
    "            best_estimator_ = clone(gs.best_estimator_)\n",
    "            del gs\n",
    "            gc.collect()    \n",
    "            return best_estimator_\n",
    "\n",
    "        def _hp_tune_v2(model, grid1, grid2, grid3, X_tr, y_tr, X_ho, y_ho, cv, scoring): \n",
    "            fit_params={'early_stopping_rounds':10,\\\n",
    "                        'eval_set':[(X_ho, y_ho)],\\\n",
    "                        'verbose':0}\n",
    "            gs = GridSearchCV(model, param_grid=grid1, cv = cv, scoring=scoring, n_jobs=-1, verbose=1)\n",
    "            gs.fit(X_tr, y_tr, **fit_params)    \n",
    "            bp = gs.best_params_\n",
    "            model = model.set_params(**bp)\n",
    "            del gs\n",
    "            gc.collect()\n",
    "\n",
    "            gs = GridSearchCV(model,param_grid = grid2, cv = cv, scoring = scoring, n_jobs=-1, verbose = 1)\n",
    "            gs.fit(X_tr, y_tr, **fit_params)    \n",
    "            bp.update(gs.best_params_)\n",
    "            model = model.set_params(**bp)\n",
    "            del gs\n",
    "            gc.collect()\n",
    "            bp_c = bp.copy()\n",
    "\n",
    "            best_score = -np.inf\n",
    "            for params in tqdm_notebook(list(ParameterGrid(grid3))):\n",
    "                bp_c.update(params)\n",
    "                model = model.set_params(**bp_c)\n",
    "                mean_cv_score = cross_val_score(model, X_tr, y_tr, cv=cv, scoring =scoring, n_jobs=-1).mean()\n",
    "                if mean_cv_score>best_score:\n",
    "                    best_score = mean_cv_score            \n",
    "                    best_estimator_ = model\n",
    "                else:\n",
    "                    break\n",
    "            return clone(best_estimator_)        \n",
    "        ##############################################################################################################\n",
    "        \n",
    "        if not(self.to_tune):\n",
    "            self.best_model = self.model\n",
    "            self.mean_cv_score = cross_val_score(self.best_model, X_tr, y_tr, cv=self.cv, scoring=self.cv_scoring).mean()\n",
    "            self.best_model.fit(X_tr, y_tr)\n",
    "            if self.predict_proba:\n",
    "                self.ho_score = self.ho_scoring_func(y_ho, self.best_model.predict_proba(X_ho)[:, 1])\n",
    "            else:\n",
    "                self.ho_score = self.ho_scoring_func(y_ho, self.best_model.predict(X_ho))                                             \n",
    "        else:\n",
    "            if type(self.model).__name__ in ('DecisionTreeRegressor', 'ExtraTreeRegressor'):\n",
    "                tree_pg = {'max_depth':np.arange(7, 41), 'min_samples_leaf':[2, 20, 200]}\n",
    "                self.best_model = _hp_tune_v1(self.model, tree_pg, X_tr, y_tr, cv=self.cv, scoring=self.cv_scoring)\n",
    "                \n",
    "                self.mean_cv_score = cross_val_score(self.best_model, X_tr, y_tr, cv=self.cv, scoring=self.cv_scoring).mean()\n",
    "                self.best_model.fit(X_tr, y_tr)\n",
    "                self.ho_score = self.ho_scoring_func(y_ho, self.best_model.predict(X_ho))\n",
    "            elif type(self.model).__name__ in ('RandomForestRegressor', 'RandomForestClassifier',\\\n",
    "                                               'ExtraTreesRegressor', 'ExtraTreesClassifier'):\n",
    "                init_params = self.model.get_params()\n",
    "                trees_pg = {'max_depth':np.arange(5, 21),'min_samples_leaf':[2, 20],'n_estimators':[10],\\\n",
    "                            'n_jobs':[-1], 'random_state':[init_params['random_state']]}\n",
    "                self.best_model = _hp_tune_v1(self.model,trees_pg, X_tr, y_tr, cv=self.cv, scoring=self.cv_scoring)\n",
    "                bp = init_params\n",
    "                best_par = self.best_model.get_params()\n",
    "                del best_par['n_estimators']\n",
    "                bp.update(**best_par)\n",
    "                self.best_model = self.best_model.set_params(**bp)\n",
    "                \n",
    "                self.mean_cv_score = cross_val_score(self.best_model, X_tr, y_tr, cv=self.cv, scoring=self.cv_scoring).mean()                \n",
    "                self.best_model.fit(X_tr, y_tr)\n",
    "                self.ho_score = self.ho_scoring_func(y_ho, self.best_model.predict(X_ho))\n",
    "                \n",
    "            elif type(self.model).__name__ in ('LGBMRegressor', 'LGBMClassifier'): \n",
    "                init_params = self.model.get_params()\n",
    "                lgb_grid1 = {'n_estimators':[10], 'n_jobs':[-1], 'random_state':[init_params['random_state']],\\\n",
    "                             'max_depth':np.arange(4, 21).tolist(),\\\n",
    "                             'num_leaves':[32, 64, 128, 256, 512, 1024],\\\n",
    "                             'min_child_samples':[20, 50]}\n",
    "                lgb_grid2 = {'subsample':np.linspace(.1, 1, 10),\\\n",
    "                             'colsample_bytree':np.linspace(.1, 1, 10)}\n",
    "                lgb_grid3 = {'learning_rate':np.linspace(.01, .1, 10), 'n_estimators':[init_params['n_estimators']]}\n",
    "    \n",
    "                self.best_model = _hp_tune_v2(LGBMRegressor(n_jobs=-1),\\\n",
    "                                            lgb_grid1, lgb_grid2, lgb_grid3,\\\n",
    "                                            X_tr, y_tr, X_ho, y_ho,\\\n",
    "                                            cv=self.cv, scoring=self.cv_scoring)\n",
    "                lr = self.best_model.get_params()['learning_rate']\n",
    "                gs = GridSearchCV(self.best_model,\\\n",
    "                               param_grid = {'learning_rate':np.linspace(lr-.09,lr+.09, 10)},\\\n",
    "                               cv=self.cv, scoring=self.cv_scoring, verbose = 1)\n",
    "                gs.fit(X_tr, y_tr)\n",
    "                self.best_model = gs.best_estimator_                    \n",
    "                self.mean_cv_score = cross_val_score(self.best_model, X_tr, y_tr, cv=self.cv, scoring=self.cv_scoring).mean()                \n",
    "                self.best_model.fit(X_tr, y_tr)\n",
    "                self.ho_score = self.ho_scoring_func(y_ho, self.best_model.predict(X_ho))\n",
    "                \n",
    "            elif type(self.model).__name__ in ('XGBRegressor', 'XGBClassifier'): \n",
    "                init_params = self.model.get_params()\n",
    "                xgb_grid1 = {'n_estimators':[10], 'n_jobs':[-1], 'random_state':[init_params['random_state']],\\\n",
    "                             'max_depth':np.arange(4, 21).tolist(),\\\n",
    "                             'min_child_weight':[20, 50]}\n",
    "                xgb_grid2 = {'subsample':np.linspace(.1, 1, 10),\\\n",
    "                             'colsample_bytree':np.linspace(.1, 1, 10)}\n",
    "                xgb_grid3 = {'learning_rate':np.linspace(.01, .1, 10), 'n_estimators':[init_params['n_estimators']]}\n",
    "    \n",
    "                self.best_model = _hp_tune_v2(XGBRegressor(n_jobs=-1),\\\n",
    "                                            xgb_grid1, xgb_grid2, xgb_grid3,\\\n",
    "                                            X_tr, y_tr, X_ho, y_ho,\\\n",
    "                                            cv=self.cv, scoring=self.cv_scoring)\n",
    "                lr = self.best_model.get_params()['learning_rate']\n",
    "                gs = GridSearchCV(self.best_model,\\\n",
    "                               param_grid = {'learning_rate':np.linspace(lr-.09,lr+.09, 10)},\\\n",
    "                               cv=self.cv, scoring=self.cv_scoring, verbose = 1)\n",
    "                gs.fit(X_tr, y_tr)\n",
    "                self.best_model = gs.best_estimator_                    \n",
    "                self.mean_cv_score = cross_val_score(self.best_model, X_tr, y_tr, cv=self.cv, scoring=self.cv_scoring).mean()                \n",
    "                self.best_model.fit(X_tr, y_tr)\n",
    "                self.ho_score = self.ho_scoring_func(y_ho, self.best_model.predict(X_ho))\n",
    "            elif type(self.model).__name__ in ('LinearRegression', 'Lasso', 'Ridge'):\n",
    "                pg = {'alpha':[.1, .3, .7, 1, 10, 30, 70]}\n",
    "                self.best_model = _hp_tune_v1(self.model, pg, X_tr, y_tr, cv=self.cv, scoring=self.cv_scoring)\n",
    "                \n",
    "                self.mean_cv_score = cross_val_score(self.best_model, X_tr, y_tr, cv=self.cv, scoring=self.cv_scoring).mean()\n",
    "                self.best_model.fit(X_tr, y_tr)\n",
    "                self.ho_score = self.ho_scoring_func(y_ho, self.best_model.predict(X_ho))\n",
    "            elif type(self.model).__name__ in ('LinearSVR'):\n",
    "                pg = {'C':[.1, .3, .7, 1, 10, 30, 70]}\n",
    "                self.best_model = _hp_tune_v1(self.model, pg, X_tr, y_tr, cv=self.cv, scoring=self.cv_scoring)\n",
    "                \n",
    "                self.mean_cv_score = cross_val_score(self.best_model, X_tr, y_tr, cv=self.cv, scoring=self.cv_scoring).mean()\n",
    "                self.best_model.fit(X_tr, y_tr)\n",
    "                self.ho_score = self.ho_scoring_func(y_ho, self.best_model.predict(X_ho))\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.best_model.predict(X) \n",
    "            \n",
    "class SklearnHelperMulticollinearityReducer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, treshold):\n",
    "        self.treshold = treshold\n",
    "    def fit(self, X, y=None):\n",
    "        assert isinstance(X, np.ndarray)\n",
    "        D_multicoll = defaultdict(list)\n",
    "        for i in tqdm_notebook(range(X.shape[1])):\n",
    "            for j in range(i+1, X.shape[1]):\n",
    "                x1, x2 = X[:, i], X[:, j]\n",
    "                corr_x1_x2 = np.abs(np.corrcoef(x1, x2)).min()\n",
    "                if corr_x1_x2>=self.treshold:\n",
    "                    D_multicoll[i].append(j) \n",
    "        L_multicoll = []\n",
    "        for k, v in D_multicoll.items():\n",
    "            v.append(k)\n",
    "            L_multicoll.append(v)\n",
    "        del D_multicoll\n",
    "        gc.collect()\n",
    "        if len(L_multicoll)>0:\n",
    "            self.to_drop = []\n",
    "            for idxs in L_multicoll:\n",
    "                _df = pd.DataFrame(X[:, idxs]).astype(float)                \n",
    "                corr_w_target_abs = _df.corrwith(pd.Series(y)).abs()\n",
    "                self.to_drop.extend(corr_w_target_abs.drop(corr_w_target_abs.idxmax()).index.tolist())\n",
    "            self.to_drop = np.unique(self.to_drop)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        try:\n",
    "            return np.delete(X, self.to_drop, axis = 1)\n",
    "        except:\n",
    "            return X\n",
    "        \n",
    "class SklearnHelperMetaFeaturesRegressor_v2(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model, seed, n_iterations, nfolds, subsamples, path_to_folder):\n",
    "        self.n_iterations=n_iterations\n",
    "        self.model = model\n",
    "        self.seed = seed\n",
    "        self.nfolds = nfolds\n",
    "        self.subsamples = subsamples\n",
    "        self.path_to_folder = path_to_folder\n",
    "    def fit(self, X, y=None):\n",
    "        if not os.path.exists(self.path_to_folder):\n",
    "            os.makedirs(self.path_to_folder)\n",
    "        else:\n",
    "            shutil.rmtree(self.path_to_folder)\n",
    "            os.makedirs(self.path_to_folder)\n",
    "        nrows, ncols = X.shape\n",
    "        self.X = X.copy()\n",
    "        self.Z = np.zeros((nrows, self.n_iterations))\n",
    "        for i in tqdm_notebook(range(self.n_iterations)):\n",
    "            current_seed = i+self.seed\n",
    "            np.random.seed(current_seed)    \n",
    "            kf = KFold(self.nfolds, shuffle=True, random_state = current_seed)    \n",
    "            feat_to_use = np.random.choice(a = np.arange(ncols),\\\n",
    "                                           size = np.int32(np.around(np.random.choice(self.subsamples)*ncols)),\\\n",
    "                                           replace = False)\n",
    "            idx_permutation = np.random.permutation(np.arange(nrows))\n",
    "\n",
    "            _X_subspace = X[idx_permutation][:, feat_to_use]\n",
    "            _y = y[idx_permutation]\n",
    "\n",
    "            for j, (tr_idx, val_idx) in enumerate(kf.split(_X_subspace, _y)):\n",
    "                self.model.fit(_X_subspace[tr_idx], _y[tr_idx])                \n",
    "                \n",
    "                current_path = os.path.join(self.path_to_folder, f'model_{i}_{j}.pkl')\n",
    "                joblib.dump([self.model, feat_to_use], current_path)\n",
    "                \n",
    "                self.Z[val_idx, i] = self.model.predict(_X_subspace[val_idx])\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        if isinstance(self.X, (np.ndarray, np.generic)):\n",
    "            if np.array_equal(self.X, X, equal_nan=True):\n",
    "                return self.Z\n",
    "        elif isinstance(self.X, (csc_matrix)):\n",
    "            if np.array_equal(self.X[:,0].toarray().flatten(), X[:,0].toarray().flatten(), equal_nan=True):\n",
    "                return self.Z\n",
    "        L_predictitons=[]\n",
    "        for i, filename in enumerate(os.listdir(self.path_to_folder)):\n",
    "            path_to_model= os.path.join(self.path_to_folder, filename) \n",
    "            fitted_model, feat_to_use = joblib.load(path_to_model)\n",
    "            L_predictitons.append(fitted_model.predict(X[:, feat_to_use]).flatten().reshape(-1,1))\n",
    "        return np.column_stack([np.mean(np.column_stack(arr), 1)\\\n",
    "                                for arr in np.split(np.array(L_predictitons), self.n_iterations)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
